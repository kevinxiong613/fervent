{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Preprocessing\n",
    "Rename the columns as well as change positive tweets from target = 4 to target = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wh/j0mpnsjn5dn3d140byhbb9h80000gn/T/ipykernel_23981/2755469249.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../model-files/tweets-cleaned.csv\", encoding='ISO-8859-1')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>URL aww bummer shoulda got david carr third day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>dived many time ball managed save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>behaving mad see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700025</th>\n",
       "      <td>4</td>\n",
       "      <td>chris great hear due time amp reminder indeed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700026</th>\n",
       "      <td>4</td>\n",
       "      <td>thanks shout great aboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700027</th>\n",
       "      <td>4</td>\n",
       "      <td>hey long URL talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700028</th>\n",
       "      <td>4</td>\n",
       "      <td>matt would say URL adulthood URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700029</th>\n",
       "      <td>4</td>\n",
       "      <td>could say egg face</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700030 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        target                                              clean\n",
       "0            0    URL aww bummer shoulda got david carr third day\n",
       "1            0  upset update facebook texting might cry result...\n",
       "2            0    dived many time ball managed save rest go bound\n",
       "3            0                    whole body feel itchy like fire\n",
       "4            0                                   behaving mad see\n",
       "...        ...                                                ...\n",
       "1700025      4  chris great hear due time amp reminder indeed ...\n",
       "1700026      4                          thanks shout great aboard\n",
       "1700027      4                                  hey long URL talk\n",
       "1700028      4                   matt would say URL adulthood URL\n",
       "1700029      4                                 could say egg face\n",
       "\n",
       "[1700030 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../model-files/tweets-cleaned.csv\", encoding='ISO-8859-1')\n",
    "df = df.iloc[:, [0, 2]] # Don't want the data from before processing\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>URL aww bummer shoulda got david carr third day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>dived many time ball managed save rest go bound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>behaving mad see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700025</th>\n",
       "      <td>4</td>\n",
       "      <td>chris great hear due time amp reminder indeed ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700026</th>\n",
       "      <td>4</td>\n",
       "      <td>thanks shout great aboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700027</th>\n",
       "      <td>4</td>\n",
       "      <td>hey long URL talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700028</th>\n",
       "      <td>4</td>\n",
       "      <td>matt would say URL adulthood URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1700029</th>\n",
       "      <td>4</td>\n",
       "      <td>could say egg face</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700030 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                              clean\n",
       "0               0    URL aww bummer shoulda got david carr third day\n",
       "1               0  upset update facebook texting might cry result...\n",
       "2               0    dived many time ball managed save rest go bound\n",
       "3               0                    whole body feel itchy like fire\n",
       "4               0                                   behaving mad see\n",
       "...           ...                                                ...\n",
       "1700025         4  chris great hear due time amp reminder indeed ...\n",
       "1700026         4                          thanks shout great aboard\n",
       "1700027         4                                  hey long URL talk\n",
       "1700028         4                   matt would say URL adulthood URL\n",
       "1700029         4                                 could say egg face\n",
       "\n",
       "[1700030 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].replace(4, 1, inplace=True) # Replace all the 4s with 1s\n",
    "df.rename(columns={'target': 'sentiment', 'text':'tweet'}, inplace=True) # Rename the columns\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    object\n",
       "clean        object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['URL', 'aww', 'bummer', 'shoulda', 'got', 'david', 'carr', 'third', 'day']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text, sentiment = list(df['clean']), list(df['sentiment']) # Turn these both into lists\n",
    "\n",
    "text = [str(item) for item in text] \n",
    "positive_text = text[850000:] # Dataset arranged in a way such that the first half is negative and second half is positive\n",
    "negative_text = text[:850000]\n",
    "positive_sentiment = sentiment[850000:]\n",
    "negative_sentiment = sentiment[:850000]\n",
    "negative_text[0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use One-Hot-Encoding\n",
    "\n",
    "We will be considering the top X frequent Tweets for both categories and doing one-hot-encoding with them. Basically, it will be a matrix where each row represents a message and each column represents each X most frequent words. If that particular word is present, mark it as 1. Otherwise, words that aren't in that message are marked with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONT RUN AGAIN AFTER WE PICKLED THE VECTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the frequencies up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../model-files/stop-words.txt\") as f:\n",
    "    stopwords = [line.strip() for line in f]\n",
    "stopwords = set(stopwords) # Set of stop words to check against for the clean method for each prediction, keep in mind this model takes in cleaned data so should only be used for prediction\n",
    "\n",
    "\n",
    "negative_word_count = 0 # Total number of words for each class\n",
    "positive_word_count = 0\n",
    "positive_words = {}  # Word frequencies for each class\n",
    "negative_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(positive_text)):\n",
    "    splitted = positive_text[i].split()\n",
    "    for j in range(len(splitted)):\n",
    "        if splitted[j] not in positive_words: # Keep track of \n",
    "            positive_words[splitted[j]] = 0\n",
    "        positive_words[splitted[j]] += 1\n",
    "        positive_word_count += 1\n",
    "\n",
    "for i in range(len(negative_text)):\n",
    "    splitted = negative_text[i].split()\n",
    "    for j in range(len(splitted)):\n",
    "        if splitted[j] not in negative_words:\n",
    "            negative_words[splitted[j]] = 0\n",
    "        negative_words[splitted[j]] += 1\n",
    "        negative_word_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grab the top 1000 most frequent words with significant meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(64292, 'got'), (73334, 'lol'), (77178, 'thanks'), (78728, 'get'), (80260, 'like'), (90660, 'quot'), (106220, 'love'), (118700, 'day'), (131674, 'good'), (528914, 'URL')]\n",
      "[(69096, 'going'), (71748, 'want'), (78616, 'got'), (78866, 'today'), (86172, 'like'), (95790, 'work'), (98176, 'get'), (99002, 'go'), (107166, 'day'), (470392, 'URL')]\n"
     ]
    }
   ],
   "source": [
    "most_freq_pos = list()\n",
    "for k, v in positive_words.items():\n",
    "    most_freq_pos.append((v, k))\n",
    "most_freq_neg = list()\n",
    "for k, v in negative_words.items():\n",
    "    most_freq_neg.append((v, k))\n",
    "\n",
    "most_freq_pos.sort()\n",
    "most_freq_neg.sort() \n",
    "# We're gonna take top 1000 unique words, but if the lists have the same word at the same index, don't use it\n",
    "print(most_freq_pos[-10:])\n",
    "print(most_freq_neg[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "{'yummy', 'crazy', 'making', 'cuz', 'suck', 'say', 'facebook', 'buddy', 'cool', 'screen', 'mail', 'worst', 'starting', 'hahaha', 'st', 'read', 'hopefully', 'care', 'awake', 'house', 'yay', 'moon', 'pc', 'awful', 'yes', 'minute', 'stuck', 'lakers', 'men', 'sit', 'understand', 'doesnt', 'perfect', 'drive', 'seeing', 'call', 'learn', 'blog', 'shift', 'photo', 'ago', 'plan', 'anything', 'people', 'thanks', 'stop', 'fast', 'ipod', 'back', 'reason', 'fuck', 'soo', 'thinking', 'even', 'coming', 'drunk', 'rain', 'drink', 'track', 'working', 'btw', 'cd', 'everyone', 'dance', 'bro', 'gutted', 'youu', 'said', 'air', 'eat', 'else', 'nap', 'terrible', 'felt', 'shoot', 'tomorrow', 'friend', 'change', 'wit', 'thats', 'worth', 'well', 'aww', 'ya', 'twitter', 'full', 'friday', 'went', 'needed', 'dad', 'used', 'chocolate', 'nd', 'answer', 'haha', 'soon', 'huge', 'shopping', 'scary', 'fact', 'date', 'holiday', 'stomach', 'sunday', 'yum', 'enjoying', 'min', 'moving', 'tom', 'studying', 'annoying', 'info', 'finish', 'road', 'appreciate', 'beach', 'especially', 'cousin', 'ball', 'oh', 'miley', 'point', 'later', 'anyone', 'shout', 'worse', 'view', 'flight', 'hmm', 'left', 'kate', 'job', 'france', 'broke', 'beat', 'turned', 'group', 'fresh', 'cover', 'wtf', 'exciting', 'quot', 'star', 'currently', 'bar', 'mood', 'might', 'graduation', 'yr', 'forward', 'hospital', 'kiss', 'jealous', 'really', 'blah', 'couldnt', 'pls', 'problem', 'sweet', 'quite', 'ticket', 'comment', 'sent', 'ok', 'anymore', 'em', 'hanging', 'hungry', 'pm', 'broken', 'secret', 'love', 'peep', 'busy', 'must', 'myspace', 'ever', 'worried', 'stupid', 'aw', 'pool', 'ear', 'wonderful', 'add', 'took', 'always', 'background', 'breakfast', 'as', 'woot', 'bye', 'message', 'chicken', 'stay', 'wonder', 'big', 'one', 'loved', 'web', 'due', 'woke', 'ha', 'aint', 'today', 'yeah', 'planning', 'pretty', 'around', 'come', 'wont', 'google', 'bed', 'david', 'bored', 'alright', 'played', 'tweet', 'cat', 'share', 'would', 'wishing', 'try', 'black', 'stopped', 'man', 'kind', 'freaking', 'late', 'line', 'mention', 'made', 'boy', 'cream', 'fantastic', 'watch', 'better', 'exactly', 'play', 'mile', 'fever', 'amp', 'playing', 'paper', 'sister', 'depressed', 'excited', 'chance', 'much', 'profile', 'hr', 'airport', 'least', 'finally', 'peace', 'dude', 'award', 'hour', 'okay', 'except', 'gt', 'gym', 'sadly', 'everybody', 'still', 'sunshine', 'state', 'office', 'wear', 'many', 'chill', 'bitch', 'clothes', 'sound', 'hun', 'asleep', 'completely', 'heart', 'know', 'nobody', 'mall', 'wedding', 'july', 'park', 'congratulation', 'met', 'sick', 'headache', 'month', 'vote', 'sat', 'others', 'along', 'packing', 'idk', 'living', 'yesterday', 'oo', 'another', 'last', 'crap', 'run', 'listen', 'shirt', 'disappointed', 'since', 'city', 'taken', 'going', 'believe', 'club', 'hurt', 'page', 'exhausted', 'ppl', 'yea', 'pic', 'cake', 'havent', 'rest', 'dunno', 'whole', 'tired', 'case', 'sure', 'th', 'arm', 'choice', 'gosh', 'want', 'looking', 'summer', 'lovely', 'study', 'closed', 'body', 'afford', 'pizza', 'using', 'nope', 'mommy', 'hang', 'xoxo', 'interesting', 'getting', 'fix', 'ouch', 'raining', 'note', 'tweeting', 'great', 'far', 'monday', 'night', 'final', 'wish', 'computer', 'load', 'tummy', 'sad', 'thing', 'fell', 'warm', 'drinking', 'bloody', 'person', 'good', 'leg', 'phone', 'sleepy', 'burnt', 'isnt', 'super', 'set', 'finished', 'reading', 'go', 'record', 'spend', 'email', 'running', 'dying', 'best', 'poor', 'short', 'evening', 'forever', 'bike', 'ahead', 'luck', 'weird', 'drop', 'shower', 'sleep', 'check', 'exam', 'bout', 'bummer', 'day', 'cant', 'battery', 'win', 'boyfriend', 'every', 'mean', 'visit', 'english', 'fly', 'online', 'whats', 'jon', 'congrats', 'fan', 'cry', 'mtv', 'grr', 'red', 'following', 'project', 'near', 'old', 'wondering', 'week', 'first', 'someone', 'bit', 'lonely', 'power', 'self', 'luv', 'year', 'join', 'special', 'post', 'staying', 'blue', 'use', 'something', 'happened', 'green', 'light', 'birthday', 'may', 'depressing', 'although', 'arrived', 'teeth', 'woohoo', 'behind', 'woo', 'com', 'huh', 'figure', 'amazing', 'glad', 'everything', 'cheer', 'party', 'number', 'guitar', 'school', 'please', 'cancelled', 'dvd', 'absolutely', 'noo', 'idea', 'unfortunately', 'woman', 'shop', 'smile', 'anyway', 'help', 'argh', 'parent', 'hair', 'dead', 'ly', 'ah', 'hahahaha', 'hard', 'open', 'low', 'ask', 'twilight', 'tea', 'thanx', 'hear', 'tweeps', 'thx', 'get', 'feel', 'pick', 'lose', 'iphone', 'bought', 'class', 'followfriday', 'version', 'yo', 'easy', 'eh', 'past', 'future', 'tell', 'pas', 'ff', 'lol', 'hot', 'plane', 'water', 'new', 'london', 'ooh', 'slept', 'missing', 'fucking', 'vacation', 'proud', 'sore', 'morning', 'single', 'lt', 'art', 'bill', 'fall', 'lately', 'book', 'box', 'album', 'daughter', 'seems', 'shoe', 'mac', 'pissed', 'bday', 'meet', 'wife', 'history', 'ive', 'order', 'leaving', 'dress', 'fml', 'move', 'second', 'quote', 'hello', 'turn', 'boring', 'true', 'hahah', 'stuff', 'seem', 'pay', 'sale', 'though', 'shall', 'look', 'feeling', 'fam', 'thank', 'kinda', 'vega', 'goin', 'talking', 'way', 'touch', 'life', 'store', 'mum', 'chat', 'gone', 'driving', 'nite', 'find', 'cause', 'camera', 'til', 'finger', 'tuesday', 'spent', 'omg', 'smell', 'flu', 'safe', 'radio', 'hehe', 'thursday', 'mom', 'died', 'ughh', 'white', 'article', 'lost', 'outside', 'two', 'world', 'head', 'without', 'news', 'cup', 'knew', 'dang', 'story', 'horrible', 'cut', 'homework', 'cute', 'picture', 'wrong', 'nice', 'name', 'sitting', 'game', 'missed', 'didnt', 'waking', 'roll', 'see', 'done', 'weather', 'question', 'film', 'wan', 'enough', 'happen', 'college', 'top', 'company', 'upset', 'country', 'dancing', 'de', 'leave', 'bad', 'thought', 'free', 'alone', 'tried', 'forget', 'jonas', 'pleasure', 'started', 'town', 'sims', 'hug', 'blood', 'girl', 'agree', 'test', 'upload', 'beautiful', 'rainy', 'gift', 'meant', 'longer', 'excellent', 'fine', 'got', 'trailer', 'gettin', 'face', 'supposed', 'added', 'whatever', 'taste', 'welcome', 'window', 'event', 'business', 'tip', 'tonight', 'type', 'apparently', 'die', 'expensive', 'wow', 'heat', 'rip', 'real', 'goodnight', 'trek', 'concert', 'dont', 'failed', 'different', 'place', 'lmao', 'text', 'couple', 'hi', 'already', 'clean', 'ache', 'none', 'catch', 'either', 'puppy', 'lady', 'service', 'inside', 'worked', 'demi', 'random', 'three', 'badly', 'happens', 'lazy', 'next', 'sorry', 'list', 'buy', 'bus', 'actually', 'shit', 'weekend', 'posted', 'bummed', 'magic', 'ohh', 'singing', 'bbq', 'side', 'camp', 'food', 'never', 'send', 'laugh', 'car', 'train', 'keep', 'lot', 'hey', 'start', 'ta', 'put', 'revision', 'chillin', 'watchin', 'xx', 'guess', 'nose', 'losing', 'could', 'fail', 'throat', 'sunny', 'earlier', 'dm', 'room', 'nothing', 'favourite', 'wake', 'team', 'song', 'music', 'trip', 'let', 'able', 'realized', 'bb', 'fun', 'long', 'instead', 'garden', 'told', 'make', 'sexy', 'ugh', 'son', 'there', 'coffee', 'church', 'wine', 'confused', 'le', 'via', 'listening', 'bring', 'rock', 'dropped', 'door', 'im', 'mad', 'sometimes', 'lil', 'video', 'download', 'think', 'saying', 'front', 'ate', 'give', 'wait', 'break', 'ice', 'enjoy', 'tour', 'issue', 'passed', 'wanted', 'also', 'account', 'part', 'da', 'away', 'heard', 'kid', 'bug', 'looked', 'french', 'nan', 'matter', 'decided', 'wet', 'pain', 'pink', 'relaxing', 'taking', 'fav', 'walk', 'hate', 'god', 'fair', 'apple', 'hope', 'card', 'ran', 'bless', 'small', 'update', 'live', 'episode', 'twitpic', 'saturday', 'heading', 'meeting', 'seriously', 'app', 'mr', 'eye', 'right', 'lucky', 'sign', 'ur', 'baby', 'sigh', 'half', 'brother', 'remember', 'link', 'goodbye', 'bet', 'dinner', 'need', 'age', 'fat', 'slow', 'awesome', 'trouble', 'guy', 'season', 'movie', 'band', 'website', 'math', 'babe', 'cold', 'probably', 'writing', 'xd', 'ill', 'heyy', 'tho', 'vip', 'write', 'site', 'save', 'tear', 'tweetdeck', 'almost', 'foot', 'work', 'boo', 'shot', 'high', 'officially', 'gon', 'bc', 'sharing', 'death', 'home', 'totally', 'little', 'seen', 'course', 'reply', 'co', 'si', 'sing', 'tv', 'maybe', 'waiting', 'sun', 'traffic', 'mother', 'hell', 'knee', 'cried', 'indeed', 'trying', 'family', 'lunch', 'beer', 'june', 'watched', 'dog', 'dear', 'ended', 'quick', 'enjoyed', 'na', 'sleeping', 'sold', 'fb', 'follower', 'hoping', 'mine', 'yep', 'father', 'yup', 'watching', 'yet', 'liked', 'dream', 'early', 'time', 'favorite', 'ahh', 'together', 'youtube', 'round', 'folk', 'mate', 'kill', 'extra', 'loving', 'cleaning', 'rather', 'doctor', 'hold', 'john', 'scared', 'jus', 'hand', 'glass', 'happy', 'mm', 'till', 'definitely', 'moment', 'eating', 'support', 'storm', 'follow', 'kick', 'http', 'laptop', 'funny', 'hubby', 'hill', 'forgot', 'miss', 'cheese', 'bag', 'taylor', 'deal', 'end', 'damn', 'take', 'worry', 'hit', 'interview', 'saw', 'word', 'called', 'afternoon', 'money', 'talk', 'nick', 'mind', 'wednesday', 'ready', 'la', 'dentist', 'found', 'design', 'ride', 'close', 'show'}\n"
     ]
    }
   ],
   "source": [
    "pos_index = len(most_freq_pos) - 1\n",
    "res = set()\n",
    "for neg_index in range(len(most_freq_neg)-1, -1, -1):\n",
    "    if most_freq_pos[pos_index][1] != most_freq_neg[neg_index][1]:\n",
    "        if len(res) == 999: # Only add one if we are getting close, positive has more words so use that\n",
    "            res.add(most_freq_pos[pos_index][1])\n",
    "        else:\n",
    "            res.add(most_freq_pos[pos_index][1])\n",
    "            res.add(most_freq_neg[neg_index][1])\n",
    "    pos_index -= 1\n",
    "    if len(res) == 1000:\n",
    "        break\n",
    "print(len(res))\n",
    "print(res)\n",
    "\n",
    "with open(\"../model-files/words.txt\", 'w') as file:\n",
    "    for word in res:\n",
    "        file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a new dataframe with the hot encodings for each message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m hot_encode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m     10\u001b[0m vectors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtext\u001b[49m:\n\u001b[1;32m     12\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     13\u001b[0m     vector \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_list)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "with open(\"../model-files/words.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        word_list.append(line.strip())# Order doesn't matter, but needs to stay consistent so convert the set to a list\n",
    "word_list_dict = dict()\n",
    "for i in range(len(word_list)): # Map words to their index to make the next part O(1) instead of O(N)\n",
    "    word_list_dict[word_list[i]] = i\n",
    "\n",
    "hot_encode = dict()\n",
    "vectors = list()\n",
    "for message in text:\n",
    "    tokens = message.split()\n",
    "    vector = [0] * len(word_list)\n",
    "    for token in tokens:\n",
    "        if token in word_list_dict:\n",
    "            vector[word_list_dict[token]] = 1 # We have this token so for this vector, mark it accordingly\n",
    "    vectors.append(vector)\n",
    "\n",
    "# So now each index of vectors's output is just the corresponding index in the list called \"sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('model-files/training_vectors.pkl', 'wb') as f:\n",
    "    pickle.dump(vectors, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
