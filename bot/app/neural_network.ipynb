{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Tweet Data Preprocessing\n",
    "Rename the columns as well as change positive tweets from target = 4 to target = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>1</td>\n",
       "      <td>@chriswiggin3 Chris, that's great to hear :) D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1</td>\n",
       "      <td>@RachelLiskeard Thanks for the shout-out :) It...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>1</td>\n",
       "      <td>@side556 Hey!  :)  Long time no talk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>1</td>\n",
       "      <td>@staybubbly69 as Matt would say. WELCOME TO AD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>1</td>\n",
       "      <td>@DanielOConnel18 you could say he will have eg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text\n",
       "0          1  #FollowFriday @France_Inte @PKuchly57 @Milipol...\n",
       "1          1  @Lamb2ja Hey James! How odd :/ Please call our...\n",
       "2          1  @DespiteOfficial we had a listen last night :)...\n",
       "3          1                               @97sides CONGRATS :)\n",
       "4          1  yeaaaah yippppy!!!  my accnt verified rqst has...\n",
       "...      ...                                                ...\n",
       "4995       1  @chriswiggin3 Chris, that's great to hear :) D...\n",
       "4996       1  @RachelLiskeard Thanks for the shout-out :) It...\n",
       "4997       1            @side556 Hey!  :)  Long time no talk...\n",
       "4998       1  @staybubbly69 as Matt would say. WELCOME TO AD...\n",
       "4999       1  @DanielOConnel18 you could say he will have eg...\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "# Read the json tweet files in\n",
    "with open('../model-files/positive_tweets.json', 'r') as f:\n",
    "    positive_tweets_data = [json.loads(line) for line in f]\n",
    "with open('../model-files/negative_tweets.json', 'r') as f:\n",
    "    negative_tweets_data = [json.loads(line) for line in f]\n",
    "\n",
    "nltk_positive = pd.DataFrame(positive_tweets_data)\n",
    "nltk_negative = pd.DataFrame(negative_tweets_data)\n",
    "nltk_positive['target'] = 1\n",
    "nltk_negative['target'] = 0\n",
    "\n",
    "nltk_positive = nltk_positive.loc[:, ['target', 'text']]\n",
    "nltk_negative = nltk_negative.loc[:, ['target', 'text']]\n",
    "nltk_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import the stop words to be removed\"\"\"\n",
    "with open(\"../model-files/stop-words.txt\") as f:\n",
    "    stopwords = [line.strip() for line in f]\n",
    "stopwords = set(stopwords)\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    # Convert to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # Remove URLs. Match words that begin with http, http\\S+, www, etc... and \\S+ which is just one or more non-whitespace characters\n",
    "    tweet = re.sub(r'\\shttp\\S+|\\swww\\S+|\\shttps\\S+', ' URL ', tweet, flags=re.MULTILINE)\n",
    "\n",
    "    # Do they same as above but match with anything that ends with .com, .net, or website endings\n",
    "    tweet = re.sub(r'\\S+.com\\s|\\S+.net\\s|\\S+.org\\s|\\S+.co\\s|\\S+.us\\s|\\S+.edu\\s|\\S+.me\\s|\\S+.cn\\s|\\S+.uk\\s|\\S+.cn\\s', ' URL ', tweet, flags=re.MULTILINE)\n",
    "    # Get rid of @ mentions from the tweet dataset\n",
    "    tweet = re.sub(r'@\\S+', '', tweet, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove punctuation\n",
    "    tweet = re.sub(r'\\W', ' ', tweet)\n",
    "    # Remove digits\n",
    "    tweet = re.sub(r'\\d+', '', tweet)\n",
    "\n",
    "    # Tokenize the tweet (just store each word into a list)\n",
    "    tokens = word_tokenize(tweet)\n",
    "\n",
    "    tokens = [token for token in tokens if token not in stopwords] # Keep the ones that aren't a stop word\n",
    "    # Lemmatize tokens, converting it back to its base form\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "\n",
    "    # If a word has more than 3 consecutive characters, make it 2 characters instead\n",
    "    for i in range(len(tokens)):\n",
    "        chars_to_delete = list()\n",
    "        for j in range(len(tokens[i]) - 2):\n",
    "            if tokens[i][j] == tokens[i][j+1] and tokens[i][j] == tokens[i][j+2]:\n",
    "                chars_to_delete.append(tokens[i][j])\n",
    "        tokens[i] = list(tokens[i]) # Turn it into a list to delete specified values\n",
    "        for ch in chars_to_delete: # Delete based off of the characters stored in chars_to_delete\n",
    "            tokens[i].remove(ch)\n",
    "        tokens[i] = \"\".join(tokens[i]) # Convert back into a string\n",
    "\n",
    "    tokens = [token for token in tokens if len(token) > 1] # Get rid of random letters out and about from punctuation removal\n",
    "\n",
    "    # Join tokens back into a string\n",
    "    cleaned_tweet = ' '.join(tokens)\n",
    "    return cleaned_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_positive['clean'] = nltk_positive['text'].apply(preprocess_tweet) # Make a column for the cleaned data\n",
    "nltk_negative['clean'] = nltk_negative['text'].apply(preprocess_tweet) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hopeless', 'tmr']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_text = list(nltk_positive['clean']) # Dataset arranged in a way such that the first half is negative and second half is positive\n",
    "positive_text = [str(item) for item in positive_text]\n",
    "negative_text = list(nltk_negative['clean']) # Dataset arranged in a way such that the first half is negative and second half is positive\n",
    "negative_text = [str(item) for item in negative_text]\n",
    "\n",
    "positive_sentiment = list(nltk_positive['target'])\n",
    "negative_sentiment = list(nltk_negative['target'])\n",
    "\n",
    "negative_text[0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use One-Hot-Encoding\n",
    "\n",
    "We will be considering the top X frequent Tweets for both categories and doing one-hot-encoding with them. Basically, it will be a matrix where each row represents a message and each column represents each X most frequent words. If that particular word is present, mark it as 1. Otherwise, words that aren't in that message are marked with 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DONT RUN AGAIN AFTER WE PICKLED THE VECTORS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the frequencies up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_word_count = 0 # Total number of words for each class\n",
    "positive_word_count = 0\n",
    "positive_words = {}  # Word frequencies for each class\n",
    "negative_words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(positive_text)):\n",
    "    splitted = positive_text[i].split()\n",
    "    for j in range(len(splitted)):\n",
    "        if splitted[j] not in positive_words: # Keep track of \n",
    "            positive_words[splitted[j]] = 0\n",
    "        positive_words[splitted[j]] += 1\n",
    "        positive_word_count += 1\n",
    "\n",
    "for i in range(len(negative_text)):\n",
    "    splitted = negative_text[i].split()\n",
    "    for j in range(len(splitted)):\n",
    "        if splitted[j] not in negative_words:\n",
    "            negative_words[splitted[j]] = 0\n",
    "        negative_words[splitted[j]] += 1\n",
    "        negative_word_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grab the top 500 most frequent words with significant meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(174, 'amp'), (197, 'happy'), (219, 'like'), (239, 'good'), (248, 'thank'), (249, 'day'), (288, 'follow'), (323, 'love'), (394, 'thanks'), (2252, 'URL')]\n",
      "[(150, 'day'), (150, 'one'), (150, 'sorry'), (175, 'na'), (187, 'get'), (208, 'like'), (224, 'want'), (243, 'miss'), (272, 'please'), (1823, 'URL')]\n"
     ]
    }
   ],
   "source": [
    "most_freq_pos = list()\n",
    "for k, v in positive_words.items():\n",
    "    most_freq_pos.append((v, k))\n",
    "most_freq_neg = list()\n",
    "for k, v in negative_words.items():\n",
    "    most_freq_neg.append((v, k))\n",
    "\n",
    "most_freq_pos.sort()\n",
    "most_freq_neg.sort() \n",
    "# We're gonna take top 500 unique words, but if the lists have the same word at the same index, don't use it\n",
    "print(most_freq_pos[-10:])\n",
    "print(most_freq_neg[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "{'mention', 'get', 'definitely', 'started', 'actually', 'buy', 'damn', 'dm', 'job', 'today', 'pain', 'made', 'idk', 'hate', 'fav', 'black', 'city', 'day', 'morning', 'tomorrow', 'im', 'saying', 'final', 'show', 'finally', 'dude', 'dont', 'sure', 'hour', 'rn', 'set', 'touch', 'mine', 'lot', 'know', 'favorite', 'concert', 'story', 'youth', 'enjoyed', 'playing', 'everything', 'design', 'didnt', 'plan', 'perfect', 'idea', 'fucked', 'baby', 'much', 'wait', 'back', 'community', 'full', 'gt', 'http', 'hope', 'infinite', 'poor', 'live', 'bro', 'kind', 'think', 'say', 'first', 'stop', 'bam', 'cry', 'since', 'link', 'cream', 'leaving', 'co', 'anyway', 'nothing', 'everyone', 'great', 'worry', 'wake', 'heart', 'tired', 'anyone', 'hoping', 'mean', 'hopefully', 'agree', 'person', 'ugh', 'mum', 'watch', 'lucky', 'ever', 'pretty', 'getting', 'favourite', 'mind', 'talking', 'invite', 'gutted', 'followfriday', 'thanks', 'order', 'eye', 'already', 'read', 'wow', 'flipkartfashionfriday', 'song', 'nice', 'post', 'well', 'sadly', 'ready', 'influencers', 'another', 'ill', 'find', 'yes', 'ticket', 'bored', 'asleep', 'eve', 'week', 'zayniscomingbackonjuly', 'funny', 'longer', 'appreciate', 'able', 'ah', 'congratulation', 'watching', 'thing', 'reason', 'cat', 'follow', 'going', 'sorry', 'point', 'school', 'chance', 'face', 'true', 'ice', 'trying', 'aw', 'else', 'excited', 'tho', 'girl', 'happiness', 'last', 'best', 'game', 'see', 'happy', 'thought', 'snapchat', 'cool', 'put', 'smile', 'ff', 'problem', 'via', 'short', 'okay', 'birthday', 'moment', 'thats', 'sometimes', 'dog', 'beautiful', 'opportunity', 'without', 'app', 'rest', 'cant', 'never', 'free', 'cheer', 'kid', 'travel', 'way', 'wish', 'blog', 'share', 'xx', 'kikgirl', 'coming', 'picture', 'night', 'omg', 'may', 'huhu', 'wsalelove', 'phone', 'hello', 'notice', 'wishing', 'bring', 'world', 'place', 'goodbye', 'noo', 'good', 'reply', 'cause', 'sexy', 'tell', 'said', 'ok', 'awesome', 'amazing', 'enough', 'summer', 'course', 'amp', 'waiting', 'yesterday', 'anymore', 'working', 'thankyou', 'cute', 'stay', 'btw', 'family', 'busy', 'guy', 'number', 'year', 'car', 'hair', 'wi', 'making', 'month', 'dear', 'hurt', 'london', 'though', 'dream', 'let', 'wtf', 'must', 'open', 'brain', 'ｓｅｅ', 'ha', 'online', 'forget', 'please', 'still', 'due', 'sir', 'bed', 'plz', 'happen', 'sigh', 'whole', 'real', 'something', 'fun', 'could', 'twitter', 'fair', 'bit', 'friday', 'believe', 'might', 'like', 'pic', 'sick', 'country', 'following', 'end', 'friend', 'hard', 'go', 'lost', 'ear', 'future', 'worst', 'wanted', 'proud', 'need', 'time', 'fan', 'care', 'every', 'tbh', 'give', 'little', 'harry', 'aww', 'people', 'lmao', 'enjoy', 'top', 'ur', 'af', 'used', 'stuff', 'haha', 'pleasure', 'town', 'book', 'someone', 'party', 'cold', 'holiday', 'went', 'gone', 'pm', 'info', 'sleep', 'bad', 'man', 'kinda', 'got', 'take', 'arrived', 'music', 'new', 'na', 'till', 'house', 'big', 'left', 'sa', 'mom', 'hot', 'warsaw', 'found', 'paper', 'body', 'two', 'stage', 'store', 'missed', 'th', 'view', 'also', 'side', 'seen', 'album', 'guess', 'around', 'really', 'away', 'literally', 'next', 'account', 'movie', 'support', 'hahaha', 'suck', 'list', 'far', 'miss', 'sweet', 'thank', 'chat', 'others', 'hornykik', 'sad', 'ask', 'unfortunately', 'visit', 'better', 'eat', 'money', 'looking', 'later', 'raining', 'life', 'keep', 'thx', 'done', 'scared', 'call', 'forward', 'old', 'happened', 'weekend', 'follback', 'beli', 'fuck', 'work', 'wonderful', 'alone', 'word', 'close', 'glad', 'follower', 'understand', 'change', 'stats', 'news', 'look', 'feel', 'would', 'check', 'sound', 'smiling', 'either', 'question', 'hungry', 'god', 'late', 'yeah', 'part', 'talk', 'love', 'play', 'bestfriend', 'gon', 'photo', 'food', 'luck', 'pls', 'hug', 'kik', 'forgot', 'ｍｅ', 'soo', 'boy', 'ubericecream', 'right', 'weather', 'fucking', 'early', 'one', 'justi', 'meet', 'feeling', 'super', 'maybe', 'bc', 'least', 'missing', 'wrong', 'wan', 'unfollowers', 'trip', 'help', 'shit', 'home', 'start', 'win', 'ya', 'welcome', 'make', 'terrible', 'kikhorny', 'tweet', 'leave', 'seeing', 'cut', 'team', 'anything', 'video', 'retweet', 'coffee', 'congrats', 'hey', 'light', 'remember', 'email', 'rain', 'sharing', 'fab', 'stream', 'member', 'always', 'head', 'add', 'oh', 'goodnight', 'awful', 'even', 'lt', 'yay', 'fantastic', 'try', 'fback', 'babe', 'use', 'tonight', 'ago', 'hi', 'want', 'followed', 'many', 'lovely', 'long', 'lol', 'saw', 'yet', 'finished', 'tried', 'hear', 'soon'}\n"
     ]
    }
   ],
   "source": [
    "pos_index = len(most_freq_pos) - 1\n",
    "res = set()\n",
    "for neg_index in range(len(most_freq_neg)-1, -1, -1):\n",
    "    if most_freq_pos[pos_index][1] != most_freq_neg[neg_index][1]:\n",
    "        if len(res) == 499: # Only add one if we are getting close, positive has more words so use that\n",
    "            res.add(most_freq_pos[pos_index][1])\n",
    "        else:\n",
    "            res.add(most_freq_pos[pos_index][1])\n",
    "            res.add(most_freq_neg[neg_index][1])\n",
    "    pos_index -= 1\n",
    "    if len(res) == 500:\n",
    "        break\n",
    "print(len(res))\n",
    "print(res)\n",
    "\n",
    "with open(\"../model-files/words.txt\", 'w') as file:\n",
    "    for word in res:\n",
    "        file.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wh/j0mpnsjn5dn3d140byhbb9h80000gn/T/ipykernel_27666/3745831241.py:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = nltk_positive.append(nltk_negative)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>#FollowFriday @France_Inte @PKuchly57 @Milipol...</td>\n",
       "      <td>followfriday top engaged member community week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>@Lamb2ja Hey James! How odd :/ Please call our...</td>\n",
       "      <td>hey james odd please call contact centre able ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>@DespiteOfficial we had a listen last night :)...</td>\n",
       "      <td>listen last night bleed amazing track scotland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>@97sides CONGRATS :)</td>\n",
       "      <td>congrats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>yeaaaah yippppy!!!  my accnt verified rqst has...</td>\n",
       "      <td>yeaah yippy accnt verified rqst succeed got bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0</td>\n",
       "      <td>I wanna change my avi but uSanele :(</td>\n",
       "      <td>wan na change avi usanele</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0</td>\n",
       "      <td>MY PUPPY BROKE HER FOOT :(</td>\n",
       "      <td>puppy broke foot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0</td>\n",
       "      <td>where's all the jaebum baby pictures :((</td>\n",
       "      <td>jaebum baby picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0</td>\n",
       "      <td>But but Mr Ahmad Maslan cooks too :( https://t...</td>\n",
       "      <td>mr ahmad maslan cook URL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0</td>\n",
       "      <td>@eawoman As a Hull supporter I am expecting a ...</td>\n",
       "      <td>hull supporter expecting misserable week</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text  \\\n",
       "0          1  #FollowFriday @France_Inte @PKuchly57 @Milipol...   \n",
       "1          1  @Lamb2ja Hey James! How odd :/ Please call our...   \n",
       "2          1  @DespiteOfficial we had a listen last night :)...   \n",
       "3          1                               @97sides CONGRATS :)   \n",
       "4          1  yeaaaah yippppy!!!  my accnt verified rqst has...   \n",
       "...      ...                                                ...   \n",
       "4995       0               I wanna change my avi but uSanele :(   \n",
       "4996       0                         MY PUPPY BROKE HER FOOT :(   \n",
       "4997       0           where's all the jaebum baby pictures :((   \n",
       "4998       0  But but Mr Ahmad Maslan cooks too :( https://t...   \n",
       "4999       0  @eawoman As a Hull supporter I am expecting a ...   \n",
       "\n",
       "                                                  clean  \n",
       "0        followfriday top engaged member community week  \n",
       "1     hey james odd please call contact centre able ...  \n",
       "2        listen last night bleed amazing track scotland  \n",
       "3                                              congrats  \n",
       "4     yeaah yippy accnt verified rqst succeed got bl...  \n",
       "...                                                 ...  \n",
       "4995                          wan na change avi usanele  \n",
       "4996                                   puppy broke foot  \n",
       "4997                                jaebum baby picture  \n",
       "4998                           mr ahmad maslan cook URL  \n",
       "4999           hull supporter expecting misserable week  \n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = nltk_positive.append(nltk_negative)\n",
    "text, sentiment = list(df['clean']), list(df['target']) # Turn these both into lists\n",
    "text = [str(item) for item in text]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a new array with the hot encodings for each message and sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "with open(\"../model-files/words.txt\", 'r') as file:\n",
    "    for line in file:\n",
    "        word_list.append(line.strip())# Order doesn't matter, but needs to stay consistent so convert the set to a list\n",
    "word_list_dict = dict()\n",
    "for i in range(len(word_list)): # Map words to their index to make the next part O(1) instead of O(N)\n",
    "    word_list_dict[word_list[i]] = i\n",
    "\n",
    "hot_encode = dict()\n",
    "vectors = list()\n",
    "for i in range(len(text)):\n",
    "    message = text[i]\n",
    "    tokens = message.split()\n",
    "    vector = [0] * len(word_list)\n",
    "    for token in tokens:\n",
    "        if token in word_list_dict:\n",
    "            vector[word_list_dict[token]] = 1 # We have this token so for this vector, mark it accordingly\n",
    "    vectors.append((vector, sentiment[i])) # Add the whole vector, AND the sentiment\n",
    "\n",
    "# So now each index of vectors's output is just the corresponding index in the list called \"sentiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../model-files/training_vectors.pkl', 'wb') as f:\n",
    "    pickle.dump(vectors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NeuralNet import NeuralNetwork\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 1)\n",
      "(500, 1)\n"
     ]
    }
   ],
   "source": [
    "with open('../model-files/training_vectors.pkl', 'rb') as file:\n",
    "    # Load the pickled object (in this case, an array)\n",
    "    data_vectors = pickle.load(file)\n",
    "random.shuffle(data_vectors)\n",
    "for i in range(len(data_vectors)):\n",
    "    curr_in, curr_out = data_vectors[i]\n",
    "    input = np.array(curr_in) / 255 # Turn the input into a numpy array\n",
    "    input = np.reshape(input, (-1, 1)) # Need to turn it from 1x784 to 784x1, -1 used by numpy to automatically infer the number of rows based on size of original, 1 represents I only want 1 column\n",
    "    output = np.array([curr_out])\n",
    "    data_vectors[i] = (input, output)\n",
    "train_data, test_data = data_vectors[:8500], data_vectors[8500:]\n",
    "print(train_data[0][0].shape)\n",
    "print(test_data[0][0].shape) # These should be the size of the input vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m neuralnet \u001b[38;5;241m=\u001b[39m NeuralNetwork([\u001b[38;5;241m500\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mneuralnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/fervent/bot/app/NeuralNet.py:14\u001b[0m, in \u001b[0;36mNeuralNetwork.fit\u001b[0;34m(self, train_data, test_data)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_data, test_data):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_biases\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/git/fervent/bot/app/NeuralNet.py:84\u001b[0m, in \u001b[0;36mNeuralNetwork.backpropagation\u001b[0;34m(self, weights, biases, training_data, test_data, learning_rate, epochs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, layer_count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m): \u001b[38;5;66;03m# Apply the delta values to the weights and biases before starting the next training data\u001b[39;00m\n\u001b[1;32m     83\u001b[0m         biases[layer] \u001b[38;5;241m=\u001b[39m biases[layer] \u001b[38;5;241m+\u001b[39m (learning_rate \u001b[38;5;241m*\u001b[39m delta_list[layer]) \u001b[38;5;66;03m# b = b + lambda * delta(layer)\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m         weights[layer] \u001b[38;5;241m=\u001b[39m weights[layer] \u001b[38;5;241m+\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(delta_list[layer], a[layer\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtranspose()) \u001b[38;5;66;03m# w = w + lamba * (delta(layer) x a[layer-1].transpose())\u001b[39;00m\n\u001b[1;32m     86\u001b[0m incorrect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_accuracy(test_data, weights, biases)\n\u001b[1;32m     87\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m incorrect\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(test_data) \u001b[38;5;66;03m# Adjust the learning rate based on how many we are still getting wrong\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "neuralnet = NeuralNetwork([500, 300, 100, 1])\n",
    "neuralnet.fit(train_data, test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
